<?xml version="1.0" encoding="utf-8"?> <feed xmlns="http://www.w3.org/2005/Atom"> <title>Andrew's Blog</title> <link href="https://andrewwangva.github.io/atom.xml" rel="self"/> <link href="https://andrewwangva.github.io/"/> <updated>2024-04-26T01:12:53-04:00</updated> <id>https://andrewwangva.github.io</id> <author> <name></name> <email></email> </author> <entry> <title>Using MCTS to Beat FunSearch</title> <link href="https://andrewwangva.github.io/blog/2024/FunMCTSearch/"/> <updated>2024-04-21T00:00:00-04:00</updated> <id>https://andrewwangva.github.io/blog/2024/FunMCTSearch</id> <content type="html">&lt;h1 id=&quot;previous-work&quot;&gt;Previous Work&lt;/h1&gt; &lt;p&gt;Deepmind introduced &lt;a href=&quot;https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/&quot;&gt;FunSearch&lt;/a&gt; [1], a paper that provided a unique way to generate programs that solved optimization problems. They were able to find improvements in practice to algorithmic combinatorial problems such as the Cap set problem or online bin packing. The programs generated were largely heuristic based, but due to their complexity, was able to beat out traditional heuristics created. The paper introduced an interesting idea of using LLM’s as a mutator in a genetic algorithm setup. The authors would repeatedly generate new programs by combining previous ones, creating hybrids and increasing variation. Similar ideas were built on top of the idea of using LLMs as a policy from Deepmind including AlphaGeometry.&lt;/p&gt; &lt;p&gt;Some of the results from FunSearch seemed pretty impressive, beating the state of the art methods in Cap Set and the online bin-packing problem. In this work, we chose to mainly focus on the online bin-packing problem. The problem involves sequentially placing items of varying sizes into bins with a fixed capacity in real-time, aiming to minimize the number of bins used without knowing future items. To follow FunSearch’s format of the problem, we mutate a function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;priority&lt;/code&gt; which assigns a priority to each bin given an item that is then sorted into the highest priority bin:&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;priority&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;Unfortunately, it doesn’t seem that many in the academic community have been working on improving upon this work outside of Deepmind. The results achieved, specifically in the bin-packing problem requires up to scales of 10^6 LLM calls along with multiple runs. We introduce a new method, based around MCTS in finding heuristic programs given an evaluation function that is competitive at a much smaller scale.&lt;/p&gt; &lt;h1 id=&quot;incorporating-mcts&quot;&gt;Incorporating MCTS&lt;/h1&gt; &lt;p&gt;MCTS has widely been used in AI systems for games. The idea is to use a policy to narrow down moves to search over and then to evaluate the downstream nodes by simulation and a value function. The core of the MCTS algorithm consists of four distinct steps: Selection, Expansion, Simulation, and Backpropagation. These steps are repeated iteratively to build a search tree.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/assets/img/MCTS/MCTS_figure-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/assets/img/MCTS/MCTS_figure-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/assets/img/MCTS/MCTS_figure-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/assets/img/MCTS/MCTS_figure.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $('.responsive-img-srcset').remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;Figure taken from the AlphaGo paper. Credit: Silver et al. [2]&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;In this section, we’ll briefly describe how we combined LLM outputs with MCTS. We use the LLM as a policy, prompting it with an existing solution and asking it to modify it slightly. We use the &lt;a href=&quot;https://people.brunel.ac.uk/~mastjjb/jeb/orlib/binpackinfo.html&quot;&gt;OR dataset&lt;/a&gt; as in FunSearch, we optimize against OR1 and treat OR2-OR4 as our test set.&lt;/p&gt; &lt;h3 id=&quot;1-selection&quot;&gt;1. Selection&lt;/h3&gt; &lt;p&gt;In each iteration, we choose a node to expand by traversing a tree and choosing the child with the largest Upper Confidence Bound (UCB). The UCB for a node is calculated as follows:&lt;/p&gt; \[UCB(s, a) = U(s, a) + Q(s, a)\] \[U(s, a) = \frac{V(s)}{1 + N}\] \[Q(s, a) = \sum_{s'}^{\text{simulated nodes}} \frac{V(s')}{N}\] &lt;p&gt;Where:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V(s)&lt;/code&gt; is the value of the program evaluated against our bin packing dataset.&lt;/li&gt; &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;N&lt;/code&gt; is the number of times we’ve visited or simulated a node in the subtree.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This UCB score serves as a way to tradeoff between exploration and exploitation, both increasing the breadth of our tree along with the depth when we find a strong path.&lt;/p&gt; &lt;h3 id=&quot;2-expansion&quot;&gt;2. Expansion&lt;/h3&gt; &lt;p&gt;Expansion is performed using the Large Language Model (LLM) as a policy to generate a few candidate actions. We prompt the LLM to take the existing solution from the current node and modify it slightly to improve results.&lt;/p&gt; &lt;h3 id=&quot;3-simulation&quot;&gt;3. Simulation&lt;/h3&gt; &lt;p&gt;Simulation involves using a smaller LLM as our fast rollout policy. We go through multiple rounds of modifying our code slightly as a lookahead into what it may look like after a series of modifications. In our experiments, we use CodeLlama-7b as our smaller, weaker model and GPT-3 as our larger, stronger model. We then update our values for the visit number along with our simulation score for each node in the path.&lt;/p&gt; &lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt; &lt;p&gt;First we ran an experiment comparing it to simpler search methods. We compare it to methods that repeatedly applies the improvement prompt. We also consider a simple tree where we build out a tree where each node has two children.&lt;/p&gt; &lt;div style=&quot;display: flex; justify-content: space-between;&quot;&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/assets/img/MCTS/chain-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/assets/img/MCTS/chain-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/assets/img/MCTS/chain-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/assets/img/MCTS/chain.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $('.responsive-img-srcset').remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;Self-improvement Chain&lt;/figcaption&gt; &lt;/figure&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/assets/img/MCTS/binary_tree-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/assets/img/MCTS/binary_tree-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/assets/img/MCTS/binary_tree-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/assets/img/MCTS/binary_tree.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $('.responsive-img-srcset').remove();&quot; /&gt; &lt;/picture&gt;&lt;figcaption class=&quot;caption&quot;&gt;Self-improvement Binary Tree&lt;/figcaption&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Below, we show MCTS outpeforms FunSearch when using a comparable amount of LLM calls from the genetic algorithm and is competitive with the best results Deepmind had at a much smaller scale.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Methods using 1000 LLM Calls&lt;/th&gt; &lt;th&gt;OR1&lt;/th&gt; &lt;th&gt;OR2&lt;/th&gt; &lt;th&gt;OR3&lt;/th&gt; &lt;th&gt;OR4&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Zero-shot prompting 1000 times&lt;/td&gt; &lt;td&gt;51.9&lt;/td&gt; &lt;td&gt;107.7&lt;/td&gt; &lt;td&gt;212.0&lt;/td&gt; &lt;td&gt;420.35&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;10 x Self-improvement Chain depth 100&lt;/td&gt; &lt;td&gt;51.85&lt;/td&gt; &lt;td&gt;107.7&lt;/td&gt; &lt;td&gt;212.0&lt;/td&gt; &lt;td&gt;420.35&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Binary tree with 1000 nodes&lt;/td&gt; &lt;td&gt;51.8&lt;/td&gt; &lt;td&gt;107.7&lt;/td&gt; &lt;td&gt;212.0&lt;/td&gt; &lt;td&gt;420.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;FunSearch with 1000 API Calls&lt;/td&gt; &lt;td&gt;51.75&lt;/td&gt; &lt;td&gt;107.3&lt;/td&gt; &lt;td&gt;211.2&lt;/td&gt; &lt;td&gt;418.45&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;MCTS with 1000 API Calls&lt;/td&gt; &lt;td&gt;&lt;strong&gt;51.6&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;106.0&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;207.95&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;410.85&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Method&lt;/th&gt; &lt;th&gt;OR1&lt;/th&gt; &lt;th&gt;OR2&lt;/th&gt; &lt;th&gt;OR3&lt;/th&gt; &lt;th&gt;OR4&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;MCTS with 1000 API Calls&lt;/td&gt; &lt;td&gt;&lt;strong&gt;51.6&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;106.0&lt;/td&gt; &lt;td&gt;207.95&lt;/td&gt; &lt;td&gt;410.85&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Reported FunSearch with 10^6 LLM Calls&lt;/td&gt; &lt;td&gt;51.64&lt;/td&gt; &lt;td&gt;&lt;strong&gt;105.8&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;207.46&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;410.44&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;Our method finds a better solution for the dataset it’s optimizing, OR1, but does not generalize as well - an issue that could be due to lack of hyperparameter tuning along with lacking multiple runs as FunSearch was done. We show below how our solution improves with more iterations of MCTS:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/assets/img/MCTS/MCTS_Graph-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/assets/img/MCTS/MCTS_Graph-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/assets/img/MCTS/MCTS_Graph-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/assets/img/MCTS/MCTS_Graph.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $('.responsive-img-srcset').remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;p&gt;Here’s the program our method found after 100 iterations of MCTS if you wanted it to test it out yourself:&lt;/p&gt; &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;priority&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;priority_scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;zeros_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;priority_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;priority_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.25&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;priority_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;-inf&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;priority_scores&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;h1 id=&quot;conclusion-and-limitations&quot;&gt;Conclusion and Limitations&lt;/h1&gt; &lt;p&gt;Given the limitation in resources, I’d like to scale this method - I believe it won’t take much scaling to beat FunSearch completely. Additionally, MCTS is highly parallelizable, so having access to a strong model with batch inference would greatly improve our results.&lt;/p&gt; &lt;p&gt;Additionally, there are many ways to incorporate MCTS: using an actor-critic setup to transform the problem into a two-player game or performing search on the token-level instead. We chose the simplest setup, which still proved to be quite effective.&lt;/p&gt; &lt;p&gt;In a broader scope, I hope more work is done on scaling on the dimension of inference time. As model and dataset sizes increase, an easy way to squeeze improvements in reasoning tasks with Language Model would be to use search. In many areas where we have a fast and accurate evaluation function, using search was the key to gaining strong results.&lt;/p&gt; &lt;h3 id=&quot;references&quot;&gt;References:&lt;/h3&gt; &lt;p&gt;1.Romera-Paredes, B., Barekatain, M., Novikov, A. et al. Mathematical discoveries from program search with large language models. Nature 625, 468–475 (2024).&lt;/p&gt; &lt;p&gt;2.Silver, D., Schrittwieser, J., Simonyan, K. et al. Mastering the game of Go without human knowledge. Nature 550, 354–359 (2017).&lt;/p&gt; &lt;p&gt;3.Trinh, T.H., Wu, Y., Le, Q.V. et al. Solving olympiad geometry without human demonstrations. Nature 625, 476–482 (2024).&lt;/p&gt; </content> </entry> <entry> <title>Improving CLIP Spatial Awareness Using Hard Negative Mining</title> <link href="https://andrewwangva.github.io/blog/2023/spacial-CLIP/"/> <updated>2023-12-11T00:00:00-05:00</updated> <id>https://andrewwangva.github.io/blog/2023/spacial-CLIP</id> <content type="html">&lt;h1 id=&quot;introduction-clip-doesnt-know-its-left-and-rights&quot;&gt;Introduction: CLIP doesn’t know its left and rights&lt;/h1&gt; &lt;p&gt;Multimodal learning has come into prominence recently, with text-to-image synthesis models such as DALLE or Stable Diffusion, and image-text contrastive learning models such as CLIP. In particular, CLIP has proven to be extremely useful in learning zero-shot capabilities from paired image and text data.&lt;/p&gt; &lt;p&gt;However, recent work has highlighted a common limitation in multimodal models: the ability to capture spatial relationships. Spatial relationships can be defined as how objects in an image are positioned concerning other objects. For example, A is next to B or B is on top of A. Although Language models now demonstrate an understanding of word order and spatial awareness, multimodal models still struggle to capture this relationship in both the image and captions.&lt;/p&gt; &lt;h2 id=&quot;downstream-tasks&quot;&gt;Downstream tasks&lt;/h2&gt; &lt;p&gt;Improving captioning abilities is an important building block in overcoming this limitation in all multimodal models. Creating synthetic captions from images is an already popular method in developing training data for other models such as DALLE-3. However, limitations in captioning abilities carry over to downstream tasks, and therefore, models such as DALLE-3 often also struggle to generate images from prompts that include spatial relationships. We hope that demonstrating the ability to generate spatially-aware captions will also lead to improvements in other Vision-Language models in the future.&lt;/p&gt; &lt;h2 id=&quot;semantic-similarity&quot;&gt;Semantic similarity&lt;/h2&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/assets/img/2023-11-10-spacial-CLIP/GA0Hn7zaIAAbp84-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/assets/img/2023-11-10-spacial-CLIP/GA0Hn7zaIAAbp84-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/assets/img/2023-11-10-spacial-CLIP/GA0Hn7zaIAAbp84-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/assets/img/2023-11-10-spacial-CLIP/GA0Hn7zaIAAbp84.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $('.responsive-img-srcset').remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Taken from Yamada et al. &lt;/div&gt; &lt;p&gt;CLIP is trained to maximize the similarity between embeddings of images and text. This leads to CLIP matching semantically similar images and captions but not understanding finer-grained details. Concept Association is especially an issue when there are multiple objects in an image where CLIP struggles to reason about the object’s attributes (Yamada 2022). Additionally, because of the focus on semantic similarity, CLIP also struggles with spatial relationships between objects.&lt;/p&gt; &lt;h1 id=&quot;winoground&quot;&gt;Winoground&lt;/h1&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/assets/img/2023-11-10-spacial-CLIP/winoground_example-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/assets/img/2023-11-10-spacial-CLIP/winoground_example-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/assets/img/2023-11-10-spacial-CLIP/winoground_example-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/assets/img/2023-11-10-spacial-CLIP/winoground_example.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $('.responsive-img-srcset').remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; Taken from Thrush et al. &lt;/div&gt; &lt;p&gt;Spatial awareness has been explored explicitly throughout previous literature. Thrush et al. in Winoground created an evaluation dataset that targets compositional reasoning. Each data point contains two captions and two images, where the captions contain the same words only in different orders. The difference in word ordering drastically changes the meaning of the sentence and therefore the image associated with the alternative caption also is completely different. The task then becomes to match the images to the correct captions (Thrush 2022).&lt;/p&gt; &lt;h2 id=&quot;evaluation-specifics-and-results&quot;&gt;Evaluation Specifics and Results&lt;/h2&gt; &lt;p&gt;We are going to use the image-to-caption evaluation of Winoground which aims to match captions to each image in constrast to images to captions. Different models have differnt matching strategies; CLIP uses the higher dot product similarity score when deciding which caption fits each image. Since there are in total, 4 different possible matchings out of the 2 image/caption pairs, random chance would score 25%. However, many multimodal models fail to score much higher than random chace. CLIP (ViT-B/32) scores 30.75% while the best models only score 38%.&lt;/p&gt; &lt;h2 id=&quot;spatial-examples&quot;&gt;Spatial Examples&lt;/h2&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/assets/img/2023-11-10-spacial-CLIP/Winoground_Lightbulb-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/assets/img/2023-11-10-spacial-CLIP/Winoground_Lightbulb-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/assets/img/2023-11-10-spacial-CLIP/Winoground_Lightbulb-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/assets/img/2023-11-10-spacial-CLIP/Winoground_Lightbulb.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $('.responsive-img-srcset').remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; An example of spatial image/caption pairs. Taken from Thrush et al. &lt;/div&gt; &lt;p&gt;CLIP has shown to be an extremely difficult benchmark for multimodals - and there are multitude of reasons why. First, changing the word orders creates image/caption pairs that need fine-grained reasoning capabilities to differentiate. One of the many reasoning capabilities needed to do well is spatial reasoning. We filter out 101 examples of CLIP that contain image/captions that require spatial reasoning to create a more task-speciific benchmark. Our filtering is caption-based and targets key words that may indicate spatial relationships. We will refer to this filtered out evaluation benchmark as, Winoground-Spatial.&lt;/p&gt; &lt;h1 id=&quot;hard-negative-examples&quot;&gt;Hard Negative Examples&lt;/h1&gt; &lt;p&gt;Hard negative examples are negative examples that are close to our anchor pair. These are examples that are close in some way to our positive example, but still wrong. Oftentimes, these examples are hard to distinguish from one another, and therefore cause the model trouble.&lt;/p&gt; &lt;h2 id=&quot;clip-loss&quot;&gt;CLIP Loss&lt;/h2&gt; &lt;p&gt;As a refresher on how CLIP is trained, CLIP first calculates an N by N similarity matrix from the dot products of the two embeddings. The model the calculates a loss function as the average of two cross entropies. The task becomes a classification task where we classify the correct caption for each image and the correct image for each caption, thus leading to two cross entropy functions.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/assets/img/2023-11-10-spacial-CLIP/CLIP_OG.svg-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/assets/img/2023-11-10-spacial-CLIP/CLIP_OG.svg-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/assets/img/2023-11-10-spacial-CLIP/CLIP_OG.svg-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/assets/img/2023-11-10-spacial-CLIP/CLIP_OG.svg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $('.responsive-img-srcset').remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; CLIP similarity matrix. Radford et al. &lt;/div&gt; &lt;p&gt;We modify this training procedure to include additional hard negative captions. For each image/caption pair, we generate M additional negative captions. We then calculate an N by NM similarity matrix from the dot products. Then, we only modify the loss function for image classification cross entropy function to include negative captions alongisde the original N captions. We don’t modify the caption classification cross entropy function since the negative examples don’t have a corresponding “image”.&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/assets/img/2023-11-10-spacial-CLIP/CLIP_Negative-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/assets/img/2023-11-10-spacial-CLIP/CLIP_Negative-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/assets/img/2023-11-10-spacial-CLIP/CLIP_Negative-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/assets/img/2023-11-10-spacial-CLIP/CLIP_Negative.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $('.responsive-img-srcset').remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; CLIP similarity matrix with negative examples. &lt;/div&gt; &lt;h2 id=&quot;data-and-augmentation&quot;&gt;Data and Augmentation&lt;/h2&gt; &lt;p&gt;How do we generate negative examples? We first have to create a fine-tuning dataset that contains image/caption pairs that display spatial relationships. To do this, we utilize the dataset Flickr30k, a dataset that contains 31,000 images collected from Flickr along with 5 captions annotated by human annotators. We chose this dataset due to it’s caption quality alongside the fact that many of the image/caption pairs contain multiple objects.&lt;/p&gt; &lt;p&gt;We then filter out image/caption pairs based on the captions in a similar way we created our evalutation benchmark, Winoground-Spatial. We use 20 key words and phrases such as: “left”, “on top of”, “beneath”, etc. to create a training set of roughly 3,600 examples. Although there are most likely more spatial examples, we choose this method as it is cost-effective while still ensuring the quality of the traning set being only examples of spatial relationships.&lt;/p&gt; &lt;p&gt;Data augmentations have been a commonly used as a method to prevent overfitting in image classification tasks. Although it is common to perform image augmentations, Fan et al. introduce LaCLIP to perform text augmentations on captions to create additional image/caption pairs. This method can be thought of as generating additional “positive pairs”. In order to generate text-augmentations, they utilize language models such as llama7b and GPT-3 to ensure the sentences generated are still grammatically correct. They use in-context learning and prompts such as, “Rewrite this caption of an image vividly, and keep it less than thirty words:”.&lt;/p&gt; &lt;p&gt;We follow a similar procedure to generate our negative examples. For each image/caption pair, we prompt GPT-3.5-turbo-instruct to do different augmentations. Details of the prompts are provided in the later experiments.&lt;/p&gt; &lt;h1 id=&quot;experiments&quot;&gt;Experiments&lt;/h1&gt; &lt;p&gt;For all experiments, we use a base model of CLIP(ViT-B/32) pre-trained on OpenAI’s WIT provided by OpenClip. We then use OpenAI’s API to generate augmentations. In total, the cost of generating augmentations were under $50 in credits.&lt;/p&gt; &lt;h2 id=&quot;experiment-1-switching-word-order&quot;&gt;Experiment 1: Switching word order&lt;/h2&gt; &lt;p&gt;Our first experiment explores how switching the word order may serve as hard negative examples. This method is inspired by the benchmark we are using, where the captions share the same words but in a different order. For each caption, we generate a single hard negative caption. The prompt we use is displayed below:&lt;/p&gt; &lt;figure&gt; &lt;picture&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/assets/img/2023-11-10-spacial-CLIP/GPT-word-order-480.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/assets/img/2023-11-10-spacial-CLIP/GPT-word-order-800.webp&quot; /&gt; &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/assets/img/2023-11-10-spacial-CLIP/GPT-word-order-1400.webp&quot; /&gt; &lt;!-- Fallback to the original file --&gt; &lt;img src=&quot;/assets/img/2023-11-10-spacial-CLIP/GPT-word-order.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $('.responsive-img-srcset').remove();&quot; /&gt; &lt;/picture&gt; &lt;/figure&gt; &lt;div class=&quot;caption&quot;&gt; In-context-learning prompt used to augment word order. &lt;/div&gt; &lt;p&gt;We discover adding a single hard-negative example to each example already leads to an impressive performance boost. The accuracy improves from 19.8% to a staggering 50.5% from fine-tuning.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt; &lt;/th&gt; &lt;th&gt;Pretrained CLIP&lt;/th&gt; &lt;th&gt;Word Order CLIP&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Pairs matched correctly&lt;/td&gt; &lt;td&gt;20&lt;/td&gt; &lt;td&gt;51&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Accuracy&lt;/td&gt; &lt;td&gt;0.198&lt;/td&gt; &lt;td&gt;0.505&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;We did some extra probing and noticed the majority of the improvement was from distinguishing left and right. From the additional 31 examples our fine-tuned model got correct, 18 of them were examples that the captions included the keyword of either left or right. This is consistent with our training set, where the most popular keyword of our examples is left/right.&lt;/p&gt; &lt;h2 id=&quot;experiment-2-replacing-key-spatial-words&quot;&gt;Experiment 2: Replacing key spatial words&lt;/h2&gt; &lt;p&gt;We then explore how a different augmentation workflow could impact the accuracy. In this experiment, we augment the captions to replace the keyword with another spatial keyword. For example, the keyword “on top of” could be replaced by “underneath” or “to the right of”. We again, utilize GPT to ensure the captions are still grammatically and logically correct. Because of the number of keywords avaialable, we explore how the number of negative examples during training time may affect the model’s accuracy.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt; &lt;/th&gt; &lt;th&gt;0 negative examples (Pretrained CLIP)&lt;/th&gt; &lt;th&gt;1 negative examples&lt;/th&gt; &lt;th&gt;5 negative examples&lt;/th&gt; &lt;th&gt;10 negative examples&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Pairs matched correctly&lt;/td&gt; &lt;td&gt;20&lt;/td&gt; &lt;td&gt;31&lt;/td&gt; &lt;td&gt;65&lt;/td&gt; &lt;td&gt;55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Accuracy&lt;/td&gt; &lt;td&gt;0.198&lt;/td&gt; &lt;td&gt;0.307&lt;/td&gt; &lt;td&gt;0.644&lt;/td&gt; &lt;td&gt;0.545&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;We can notice that from 0-5 negative training examples, there is a distinctive increase in model accuracy. However, an interesting result is the dropoff in accuracy from 5 training examples to 10. We did some probing into why this may be the case in the training data. One hypothesis may be the training examples for hard negatives are incorrect, in that, by a human they could be interpreted as positive examples. For example, object A could be both next to and above object B, but we are training CLIP to recognize the keyword above to be false in this case. Another hypothesis is the difficulty in training examples stunting training and needing more data. This could be case when looking at the loss function, on whether it has fully converged or not.&lt;/p&gt; &lt;h1 id=&quot;conclusion-and-limitations&quot;&gt;Conclusion and Limitations&lt;/h1&gt; &lt;p&gt;Although we have not fully tackled the issue of spatial awareness, we have made signifigant progress from our base model of CLIP, with the highest accuracy being at 64.4% compared to 19.8%. This proof of concept work shows how hard-negative examples could boost improvements in specific reasoning tasks. The concept of using these hard-negative examples are not limited to spatial relationships: it could be interesting to examine how hard negative tasks may improve other Winoground examples that require reasoning capabilities such as counting. We also note that there is a possiblity that improving the training data may not be enough, and that the architecture may need a change to fully solve spatial relationships.&lt;/p&gt; &lt;h3 id=&quot;references&quot;&gt;References:&lt;/h3&gt; &lt;p&gt;1.Robinson, J. D.; Chuang, C.-Y.; Sra, S.; Jegelka, S. Contrastive Learning with Hard Negative Samples. In Proceedings of the International Conference on Learning Representations, 2021.&lt;/p&gt; &lt;p&gt;2.Thrush Tristan, Jiang Ryan, Bartolo Max, Singh Amanpreet, Williams Adina, Kiela Douwe, and Ross Candace. 2022. Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 5238–5248.&lt;/p&gt; &lt;p&gt;3.Fan, L., Krishnan, D., Isola, P., Katabi, D., and Tian, Y. (2023a). Improving clip training with language rewrites. arXiv preprint arXiv:2305.20088.&lt;/p&gt; </content> </entry> </feed>